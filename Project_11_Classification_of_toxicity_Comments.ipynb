{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Загрузка-библиотек-и-данных\" data-toc-modified-id=\"Загрузка-библиотек-и-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Загрузка библиотек и данных</a></span></li><li><span><a href=\"#Подготовка-выборок-для-моделей\" data-toc-modified-id=\"Подготовка-выборок-для-моделей-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Подготовка выборок для моделей</a></span></li><li><span><a href=\"#Выводы-по-1-этапу\" data-toc-modified-id=\"Выводы-по-1-этапу-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Выводы по 1 этапу</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Тестирование\" data-toc-modified-id=\"Тестирование-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Тестирование</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для Интернет-магазина\n",
    "\n",
    "# Project for online store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак.\n",
    "________________________________________________________________________________________________________________________________\n",
    "The online store launches a new service. Now users can edit and supplement product descriptions, just like in wiki communities. That is, clients propose their edits and comment on the changes of others. The store needs a tool that will look for toxic comments and submit them for moderation.\n",
    "\n",
    "Train the model to classify comments into positive and negative. At your disposal is a dataset with markup on the toxicity of edits.\n",
    "\n",
    "Build a model with a quality metric *F1* of at least 0.75.\n",
    "\n",
    "**Instructions for the implementation of the project**\n",
    "\n",
    "1. Download and prepare data.\n",
    "2. Train different models.\n",
    "3. Draw conclusions.\n",
    "\n",
    "It is not necessary to use *BERT* to run the project, but you can try.\n",
    "\n",
    "**Data Description**\n",
    "\n",
    "The *text* column contains the text of the comment, and *toxic* is the target attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка библиотек и данных\n",
    "\n",
    "### Loading libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка выборок для моделей\n",
    "\n",
    "### Preparing selections for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с определения признаков и целевого признака.\n",
    "\n",
    "Let's start by defining the features and the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "def lemmatize_text(text):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['lemm_text'] = data['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['toxic']\n",
    "features = data.drop(['toxic'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее разобьем данные на выборки для обучения моделей.\n",
    "\n",
    "Next, we divide the data into samples for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid, target_train, target_valid = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.1, \n",
    "                                                                              random_state=161222)\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, \n",
    "                                                                            target_valid, \n",
    "                                                                            test_size=0.1,\n",
    "                                                                            random_state=161222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143362, 160030)\n",
      "(14337, 160030)\n",
      "(1593, 160030)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "features_train = count_tf_idf.fit_transform(features_train['lemm_text'])\n",
    "features_valid = count_tf_idf.transform(features_valid['lemm_text'])\n",
    "features_test = count_tf_idf.transform(features_test['lemm_text'])\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)\n",
    "cv_counts = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы по 1 этапу\n",
    "\n",
    "### Conclusions on stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Были выполнены следующие шаги:\n",
    "\n",
    "- Загружены данные и проведен первичный осмотр\n",
    "- Подготовлены признаки и выборки для обучения моделей\n",
    "________________________________________________________________________________________________________________________________\n",
    "The following steps have been taken:\n",
    "\n",
    "- Data uploaded and initial inspection carried out\n",
    "- Prepared features and samples for training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "\n",
    "## Education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед нами стоит задача классификации. Выберем для сравнения три модели:\n",
    "\n",
    "- LogisticRegression\n",
    "- DecisionTreeClassifier\n",
    "- CatBoostClassifier\n",
    "________________________________________________________________________________________________________________________________\n",
    "We are faced with the task of classification. Let's choose three models for comparison:\n",
    "\n",
    "- LogisticRegression\n",
    "-DecisionTreeClassifier\n",
    "- Cat Boost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for F1_score\n",
      "Best parameters set found on development set:\n",
      "{'C': 10, 'solver': 'liblinear'}\n",
      "Grid scores on development set:\n",
      "0.393789 for {'C': 0.1, 'solver': 'newton-cg'}\n",
      "0.394471 for {'C': 0.1, 'solver': 'liblinear'}\n",
      "0.685298 for {'C': 1, 'solver': 'newton-cg'}\n",
      "0.685298 for {'C': 1, 'solver': 'liblinear'}\n",
      "0.753687 for {'C': 10, 'solver': 'newton-cg'}\n",
      "0.753726 for {'C': 10, 'solver': 'liblinear'}\n",
      "\n",
      "CPU times: user 1min 58s, sys: 2min 3s, total: 4min 2s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = LogisticRegression()\n",
    "hyperparams = [{'solver':['newton-cg', 'liblinear'],\n",
    "                'C':[0.1, 1, 10]}]\n",
    "\n",
    "\n",
    "print('# Tuning hyper-parameters for F1_score')\n",
    "grid = GridSearchCV(model, hyperparams, scoring='f1',cv=cv_counts)\n",
    "grid.fit(features_train, target_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "LR_best_params = grid.best_params_\n",
    "print(LR_best_params)\n",
    "print(\"Grid scores on development set:\")\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid.cv_results_['params']):\n",
    "    print(\"%0.6f for %r\"% (mean, params))\n",
    "print()\n",
    "\n",
    "cv_f1_LR = max(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на cv 0.7537255301911344\n",
      "F1 на валидации 0.7776719375119071\n",
      "CPU times: user 7.92 s, sys: 7.55 s, total: 15.5 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.set_params(**LR_best_params)\n",
    "model.fit(features_train, target_train)\n",
    "target_predict = model.predict(features_valid)\n",
    "valid_f1_LR = f1_score(target_valid, target_predict)\n",
    "print('F1 на cv', cv_f1_LR)\n",
    "print('F1 на валидации', valid_f1_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее рассмотрим DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for F1_score\n",
      "Best parameters set found on development set:\n",
      "{'max_depth': 35, 'random_state': 161222}\n",
      "Grid scores on development set:\n",
      "0.271831 for {'max_depth': 1, 'random_state': 161222}\n",
      "0.422089 for {'max_depth': 3, 'random_state': 161222}\n",
      "0.491049 for {'max_depth': 5, 'random_state': 161222}\n",
      "0.540452 for {'max_depth': 7, 'random_state': 161222}\n",
      "0.569065 for {'max_depth': 9, 'random_state': 161222}\n",
      "0.590691 for {'max_depth': 11, 'random_state': 161222}\n",
      "0.610604 for {'max_depth': 13, 'random_state': 161222}\n",
      "0.619229 for {'max_depth': 15, 'random_state': 161222}\n",
      "0.625445 for {'max_depth': 17, 'random_state': 161222}\n",
      "0.640359 for {'max_depth': 19, 'random_state': 161222}\n",
      "0.648608 for {'max_depth': 21, 'random_state': 161222}\n",
      "0.657878 for {'max_depth': 23, 'random_state': 161222}\n",
      "0.662220 for {'max_depth': 25, 'random_state': 161222}\n",
      "0.666249 for {'max_depth': 27, 'random_state': 161222}\n",
      "0.669017 for {'max_depth': 29, 'random_state': 161222}\n",
      "0.672802 for {'max_depth': 31, 'random_state': 161222}\n",
      "0.677662 for {'max_depth': 33, 'random_state': 161222}\n",
      "0.679453 for {'max_depth': 35, 'random_state': 161222}\n",
      "\n",
      "CPU times: user 8min 49s, sys: 4.26 s, total: 8min 53s\n",
      "Wall time: 8min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "hyperparams = [{'max_depth':[x for x in range(1,36,2)],\n",
    "                'random_state':[161222]}]\n",
    "\n",
    "\n",
    "print('# Tuning hyper-parameters for F1_score')\n",
    "grid = GridSearchCV(model, hyperparams, scoring='f1',cv=cv_counts)\n",
    "grid.fit(features_train, target_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "DTC_best_params = grid.best_params_\n",
    "print(DTC_best_params)\n",
    "print(\"Grid scores on development set:\")\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid.cv_results_['params']):\n",
    "    print(\"%0.6f for %r\"% (mean, params))\n",
    "print()\n",
    "\n",
    "cv_f1_DTC = max(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на cv 0.6794530855258221\n",
      "F1 на валидации 0.6778121775025799\n",
      "CPU times: user 15.8 s, sys: 53.8 ms, total: 15.9 s\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.set_params(**DTC_best_params)\n",
    "model.fit(features_train, target_train)\n",
    "target_predict = model.predict(features_valid)\n",
    "valid_f1_DTC = f1_score(target_valid, target_predict)\n",
    "print('F1 на cv', cv_f1_DTC)\n",
    "print('F1 на валидации', valid_f1_DTC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И наконец, очередь CatBoostClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = CatBoostClassifier(verbose=False, iterations=300)\n",
    "model.fit(features_train, target_train)\n",
    "target_predict = model.predict(features_valid)\n",
    "cv_f1_CBC = cross_val_score(model,\n",
    "                            features_train, \n",
    "                            target_train, \n",
    "                            cv=cv_counts, \n",
    "                            scoring='f1').mean()\n",
    "valid_f1_CBC = f1_score(target_valid, target_predict)\n",
    "print('F1 на cv', cv_f1_CBC)\n",
    "print('F1 на валидации', valid_f1_CBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование\n",
    "\n",
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опробуем лучшую модель (LogisticRegression) на тестовых данных. Для наглядности также построим график.\n",
    "\n",
    "Let's try the best model (LogisticRegression) on the test data. For clarity, we will also build a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,10])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='RandomModel')\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.set_params(**LR_best_params)\n",
    "model.fit(features_train, target_train)\n",
    "probabilities_test = model.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_test, probabilities_one_test)\n",
    "predict_test = model.predict(features_test)\n",
    "plt.plot(fpr, tpr, label='LogisticRegression')\n",
    "print('Метрики LogisticRegression')\n",
    "print('ROC AUC:', roc_auc_score(target_test, probabilities_one_test))\n",
    "print('F1:', f1_score(target_test, predict_test))\n",
    "print('Precision:', precision_score(target_test, predict_test))\n",
    "print('Recall:', recall_score(target_test, predict_test))\n",
    "print('Accuracy:', accuracy_score(target_test, predict_test))\n",
    "print()\n",
    "\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.legend(loc='lower right', fontsize='x-large')\n",
    "\n",
    "plt.title(\"ROC-кривая\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В течение работы по проекты были сделаны следующие этапы:\n",
    "\n",
    "- Загружены и подготовлены данные.\n",
    "- Данные поделены на обучающую, валидационную и тестовою выборку.\n",
    "- Проведено сравнительное обучение различных моделей.\n",
    "- Лучшая модель проверена на тестовой выборке.\n",
    "\n",
    "При сравнительной оценке по метрике F1 лучше всего себя показал LogisticRegression. На тестовой выборке получен результат F1 = 0.78.\n",
    "\n",
    "Это говорит о том, что при примененении данной модели токсичные комментарии будут находится наиболее эффективно на практике.\n",
    "________________________________________________________________________________________________________________________________\n",
    "During the work on the project, the following stages were made:\n",
    "\n",
    "- Loaded and prepared data.\n",
    "- The data is divided into training, validation and test sets.\n",
    "- Conducted comparative training of various models.\n",
    "- The best model is tested on a test sample.\n",
    "\n",
    "In a comparative assessment of the F1 metric, LogisticRegression showed itself best of all. On the test sample, the result F1 = 0.78 was obtained.\n",
    "\n",
    "This suggests that when applying this model, toxic comments will be found most effectively in practice."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1890,
    "start_time": "2022-12-24T05:20:46.612Z"
   },
   {
    "duration": 2282,
    "start_time": "2022-12-24T05:21:08.094Z"
   },
   {
    "duration": 122,
    "start_time": "2022-12-24T05:21:42.584Z"
   },
   {
    "duration": 807,
    "start_time": "2022-12-24T05:21:45.896Z"
   },
   {
    "duration": 11,
    "start_time": "2022-12-24T05:21:48.305Z"
   },
   {
    "duration": 8,
    "start_time": "2022-12-24T05:21:53.719Z"
   },
   {
    "duration": 99354,
    "start_time": "2022-12-24T05:24:30.377Z"
   },
   {
    "duration": 28,
    "start_time": "2022-12-24T05:26:09.733Z"
   },
   {
    "duration": 35,
    "start_time": "2022-12-24T05:26:09.762Z"
   },
   {
    "duration": 928,
    "start_time": "2022-12-24T05:26:09.799Z"
   },
   {
    "duration": 103762,
    "start_time": "2022-12-24T05:26:49.879Z"
   },
   {
    "duration": 13,
    "start_time": "2022-12-24T05:28:33.643Z"
   },
   {
    "duration": 58,
    "start_time": "2022-12-24T05:28:33.658Z"
   },
   {
    "duration": 11163,
    "start_time": "2022-12-24T05:28:33.719Z"
   },
   {
    "duration": 9,
    "start_time": "2022-12-24T05:33:46.165Z"
   },
   {
    "duration": 55,
    "start_time": "2022-12-24T05:33:55.302Z"
   },
   {
    "duration": 85,
    "start_time": "2022-12-24T05:34:04.217Z"
   },
   {
    "duration": 58,
    "start_time": "2022-12-24T05:36:34.364Z"
   },
   {
    "duration": 72,
    "start_time": "2022-12-24T05:36:43.065Z"
   },
   {
    "duration": 195458,
    "start_time": "2022-12-24T05:38:26.952Z"
   },
   {
    "duration": 4,
    "start_time": "2022-12-24T05:45:49.727Z"
   },
   {
    "duration": 851,
    "start_time": "2022-12-24T05:45:50.562Z"
   },
   {
    "duration": 8,
    "start_time": "2022-12-24T05:45:51.415Z"
   },
   {
    "duration": 5,
    "start_time": "2022-12-24T05:46:11.432Z"
   },
   {
    "duration": 774,
    "start_time": "2022-12-24T05:46:15.433Z"
   },
   {
    "duration": 14,
    "start_time": "2022-12-24T05:46:19.034Z"
   },
   {
    "duration": 137958,
    "start_time": "2022-12-24T05:46:22.480Z"
   },
   {
    "duration": 60,
    "start_time": "2022-12-24T05:48:40.443Z"
   },
   {
    "duration": 63,
    "start_time": "2022-12-24T05:48:40.513Z"
   },
   {
    "duration": 18204,
    "start_time": "2022-12-24T05:48:40.580Z"
   },
   {
    "duration": 17854,
    "start_time": "2022-12-24T05:58:04.670Z"
   },
   {
    "duration": 216334,
    "start_time": "2022-12-24T05:59:55.894Z"
   },
   {
    "duration": 15887,
    "start_time": "2022-12-24T06:11:45.941Z"
   },
   {
    "duration": 17085,
    "start_time": "2022-12-24T06:12:14.105Z"
   },
   {
    "duration": 212902,
    "start_time": "2022-12-24T06:14:10.948Z"
   },
   {
    "duration": 2050,
    "start_time": "2022-12-24T17:20:45.425Z"
   },
   {
    "duration": 2361,
    "start_time": "2022-12-24T17:20:47.477Z"
   },
   {
    "duration": 12,
    "start_time": "2022-12-24T17:20:49.840Z"
   },
   {
    "duration": 93736,
    "start_time": "2022-12-24T17:20:49.854Z"
   },
   {
    "duration": 12,
    "start_time": "2022-12-24T17:22:23.592Z"
   },
   {
    "duration": 54,
    "start_time": "2022-12-24T17:22:23.606Z"
   },
   {
    "duration": 11018,
    "start_time": "2022-12-24T17:22:23.662Z"
   },
   {
    "duration": 242735,
    "start_time": "2022-12-24T17:22:34.682Z"
   },
   {
    "duration": 15517,
    "start_time": "2022-12-24T17:26:37.418Z"
   },
   {
    "duration": 534511,
    "start_time": "2022-12-24T17:26:52.938Z"
   },
   {
    "duration": 15894,
    "start_time": "2022-12-24T17:35:47.451Z"
   },
   {
    "duration": 1991,
    "start_time": "2022-12-24T23:32:32.909Z"
   },
   {
    "duration": 3855,
    "start_time": "2022-12-24T23:32:35.819Z"
   },
   {
    "duration": 14,
    "start_time": "2022-12-24T23:32:41.772Z"
   },
   {
    "duration": 111859,
    "start_time": "2022-12-24T23:32:47.718Z"
   },
   {
    "duration": 20,
    "start_time": "2022-12-24T23:34:39.585Z"
   },
   {
    "duration": 66,
    "start_time": "2022-12-24T23:34:39.607Z"
   },
   {
    "duration": 8553,
    "start_time": "2022-12-24T23:35:22.488Z"
   },
   {
    "duration": 10,
    "start_time": "2022-12-24T23:41:50.372Z"
   },
   {
    "duration": 1487,
    "start_time": "2022-12-24T23:49:40.831Z"
   },
   {
    "duration": 12,
    "start_time": "2022-12-24T23:50:40.914Z"
   },
   {
    "duration": 1313,
    "start_time": "2022-12-24T23:50:53.132Z"
   },
   {
    "duration": 14,
    "start_time": "2022-12-24T23:52:48.481Z"
   },
   {
    "duration": 1461,
    "start_time": "2022-12-24T23:52:58.813Z"
   },
   {
    "duration": 1446,
    "start_time": "2022-12-24T23:53:05.601Z"
   },
   {
    "duration": 6,
    "start_time": "2022-12-24T23:53:15.313Z"
   },
   {
    "duration": 931,
    "start_time": "2022-12-24T23:53:17.803Z"
   },
   {
    "duration": 8,
    "start_time": "2022-12-24T23:53:21.024Z"
   },
   {
    "duration": 111732,
    "start_time": "2022-12-24T23:53:24.140Z"
   },
   {
    "duration": 1436,
    "start_time": "2022-12-24T23:55:15.879Z"
   },
   {
    "duration": 975,
    "start_time": "2022-12-24T23:59:47.103Z"
   },
   {
    "duration": 37755,
    "start_time": "2022-12-24T23:59:51.479Z"
   },
   {
    "duration": 1655,
    "start_time": "2022-12-25T11:58:09.524Z"
   },
   {
    "duration": 2762,
    "start_time": "2022-12-25T11:58:11.980Z"
   },
   {
    "duration": 10,
    "start_time": "2022-12-25T11:58:17.104Z"
   },
   {
    "duration": 5,
    "start_time": "2022-12-25T12:11:51.484Z"
   },
   {
    "duration": 3,
    "start_time": "2022-12-25T12:13:25.196Z"
   },
   {
    "duration": 466,
    "start_time": "2022-12-25T12:13:29.839Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
